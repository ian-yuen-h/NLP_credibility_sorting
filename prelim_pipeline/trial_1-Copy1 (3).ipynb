{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2770094e-bede-4ba4-8bda-57dcdd9143a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get articles from elastic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "41e83f44-c5f3-439f-ae3a-270f7a8b01fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import json\n",
    "from elasticsearch.helpers import scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5548cd48-cc77-4a90-9998-99ac172a6567",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch([{'host':'localhost', 'port':9200}], http_auth=('uchicago', '3fMy7Wq90LDWBCMEXt6j'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3df2c225-29c0-4e41-a69d-73c97b031e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40681\n"
     ]
    }
   ],
   "source": [
    "res = scan(\n",
    "    es,\n",
    "    index='credibility_training_data', \n",
    "    size=5000, \n",
    "    query={'query':{'match_all':{}}}, \n",
    ")\n",
    "\n",
    "res_dict = {}\n",
    "for each in res:\n",
    "    uid = each['_id']\n",
    "    url = each['_source']['link']\n",
    "    credibility = each['_source'][\"credibility\"]\n",
    "    domain = each['_source'][\"registered_domain\"]\n",
    "    try:\n",
    "        text = each['_source'][\"text\"]\n",
    "        source = each['_source']['source']\n",
    "    except:\n",
    "        continue\n",
    "    res_dict[uid]={\"url\": url, \"source\": source, \"credibility\": credibility, \"domain\": domain, \"text\": text}\n",
    "\n",
    "print(len(res_dict))\n",
    "        \n",
    "#     # save info to a json file\n",
    "with open('data.json', 'w') as f:\n",
    "    json.dump(res_dict, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "22830d65-a518-4ffb-8732-1c6bd77cf82d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "import numpy as np\n",
    "import scipy.stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8b019e77-ba8f-46e0-a16a-d4a952879c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob')\n",
    "\n",
    "articles_attr = {}\n",
    "subjectivity_vals = np.empty(shape=[0, 1])\n",
    "polarity_vals = np.empty(shape=[0, 1])\n",
    "cred_score = np.empty(shape=[0, 1])\n",
    "id_legend = np.empty(shape=[0, 1])\n",
    "\n",
    "# subjectivity_vals_low = np.empty(shape=[0, 1])\n",
    "# polarity_vals_low = np.empty(shape=[0, 1])\n",
    "# subjectivity_vals_med = np.empty(shape=[0, 1])\n",
    "# polarity_vals_med = np.empty(shape=[0, 1])\n",
    "# subjectivity_vals_high = np.empty(shape=[0, 1])\n",
    "# polarity_vals_high = np.empty(shape=[0, 1])\n",
    "# polarity_vals_high = np.empty(shape=[0, 1])\n",
    "\n",
    "labeled_scores = {}\n",
    "\n",
    "for key, value in res_dict.items():\n",
    "    doc = nlp(value[\"text\"])\n",
    "    articles_attr[key] = {\"polarity\": doc._.polarity, \"subjectivity\": doc._.subjectivity, \"assessments\": doc._.assessments, \"source\": value[\"source\"]}\n",
    "    subjectivity_vals = np.append(subjectivity_vals, doc._.subjectivity)\n",
    "    polarity_vals = np.append(polarity_vals, doc._.polarity)\n",
    "    id_legend = np.append(id_legend, key)\n",
    "    credi_score = value[\"credibility\"]\n",
    "    cred_score = np.append(cred_score, credi_score)\n",
    "    labeled_scores[key]= credi_score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3da7830b-1713-42a1-8c0c-77a82c6ba0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of excluded:  38\n",
      "number of included:  25854\n"
     ]
    }
   ],
   "source": [
    "#excluding outliers\n",
    "subjectivity_mean = np.mean(subjectivity_vals)\n",
    "subjectivity_sd = np.std(subjectivity_vals)\n",
    "polarity_mean = np.mean(polarity_vals)\n",
    "polarity_sd = np.std(polarity_vals)\n",
    "\n",
    "id_legend_save = np.copy(id_legend)\n",
    "\n",
    "excluded = []\n",
    "excluded_index = []\n",
    "\n",
    "upper_subjectivity = subjectivity_mean + 3*subjectivity_sd\n",
    "lower_subjectivity = subjectivity_mean - 3*subjectivity_sd\n",
    "upper_polarity = polarity_mean + 3*polarity_sd\n",
    "lower_polarity = polarity_mean - 3*polarity_sd\n",
    "\n",
    "for i in range(len(id_legend_save)):\n",
    "    if (subjectivity_vals[i] > upper_subjectivity) or (subjectivity_vals[i] < lower_subjectivity) or (polarity_vals[i] > upper_polarity) or (polarity_vals[i] < lower_polarity):\n",
    "        excluded.append(id_legend_save[i])\n",
    "        excluded_index.append(i)\n",
    "        \n",
    "subjectivity_vals = np.delete(subjectivity_vals, excluded_index)\n",
    "polarity_vals = np.delete(polarity_vals, excluded_index)\n",
    "id_legend = np.delete(id_legend, excluded_index)\n",
    "cred_score = np.delete(cred_score, excluded_index)\n",
    "\n",
    "#write out the vectors, excluded or no?\n",
    "with open('vectors.txt', 'w') as f:\n",
    "    for i in range(len(cred_score)):\n",
    "        f.write(\"id: \")\n",
    "        f.write(id_legend[i].astype('str'))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"polarity: \")\n",
    "        f.write(polarity_vals[i].astype('str'))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"subjectivity: \")\n",
    "        f.write(subjectivity_vals[i].astype('str'))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"cred_score: \")\n",
    "        f.write(cred_score[i].astype('str'))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"-------------\")\n",
    "        #add publisher to above and here\n",
    "    \n",
    "\n",
    "print(\"number of excluded: \", len(excluded_index))\n",
    "print(\"number of included: \", len(id_legend))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bd0404d2-53f7-404b-9869-e01b087f823b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "def do_plot(subjectivity_vals, polarity_vals, tags, cred_score):\n",
    "    # Plot\n",
    "#     colors = [\"red\", \"green\", \"blue\"]\n",
    "    cdict = {\"high\": 'red', \"medium\": 'blue', \"low\": 'green'}\n",
    "#     fig = plt.figure(figsize=(8,8))\n",
    "    fig, ax = plt.subplots(figsize=(20,20))\n",
    "    for g in np.unique(cred_score):\n",
    "        ix = np.where(cred_score == g)\n",
    "        ax.scatter(subjectivity_vals[ix], polarity_vals[ix], c = cdict[g], label = g)\n",
    "    ax.legend()\n",
    "#     plt.scatter(subjectivity_vals, polarity_vals, c = cdict[g])\n",
    "    plt.xlabel(\"Subjectivity\")\n",
    "    plt.ylabel(\"Polarity\")\n",
    "    plt.title(\"Subjectivity-Polarity Word-level\")\n",
    "    plt.legend()\n",
    "    m, b = np.polyfit(subjectivity_vals, polarity_vals, 1) #regression\n",
    "    plt.plot(subjectivity_vals, m*subjectivity_vals + b)\n",
    "    plt.savefig(\"initial_plot.png\")\n",
    "    plt.close()\n",
    "\n",
    "do_plot(subjectivity_vals, polarity_vals, id_legend, cred_score)\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(subjectivity_vals,  polarity_vals)\n",
    "with open('stats.txt', 'w') as f:\n",
    "    f.write(\"slope: \")\n",
    "    f.write(slope.astype('str'))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"intercept: \")\n",
    "    f.write(intercept.astype('str'))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"r_value: \")\n",
    "    f.write(r_value.astype('str'))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"p_value: \")\n",
    "    f.write(p_value.astype('str'))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"std_err: \")\n",
    "    f.write(std_err.astype('str'))\n",
    "    f.write(\"-------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16ca3ff-9e77-4aac-b9ec-ec54aa91d388",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f120de2-268d-430b-bc45-a46e63c40798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37d6bad-2b62-47a3-9a5c-0e7c3a74e429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d7da02-b78e-4a0f-86af-213444904933",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # print(res.to_dict)\n",
    "# counter = 0\n",
    "# for each in res:\n",
    "#     if counter == 3:\n",
    "#         break\n",
    "#     print(each)\n",
    "#     counter += 1\n",
    "\n",
    "\n",
    "# results = []\n",
    "\n",
    "# res = es.search(\n",
    "#     index='credibility_training_data', \n",
    "#     scroll = '2m',\n",
    "#     search_type = 'scan',\n",
    "#     size=5000, \n",
    "#     body={'query':{'match_all':{}}}, \n",
    "# )\n",
    "\n",
    "# # get and return a list of urls\n",
    "# def get_urls():\n",
    "#     results = res['hits']['hits']\n",
    "#     urls = {}\n",
    "#     for u in results:\n",
    "#         url_id = u['_id']\n",
    "#         url = u['_source']['link']\n",
    "#         urls[url_id] = url\n",
    "#     return urls\n",
    "    \n",
    "# urls = get_urls()\n",
    "# len(urls)\n",
    "\n",
    "# res = es.search(\n",
    "#     index='credibility_training_data', \n",
    "#     size=5000, \n",
    "#     body={'query':{'match_all':{}}}, \n",
    "#     _source=['source', 'link']\n",
    "# )\n",
    "\n",
    "\n",
    "# results = res['hits']['hits']\n",
    "# print(len(results))\n",
    "#     counter += 1\n",
    "    \n",
    "# uid = results[0]['_id']\n",
    "# url = results[0]['_source']['link']\n",
    "# source = results[0]['_source']['source']\n",
    "# credibility = results[0]['_source'][\"credibility\"]\n",
    "# domain = results[0]['_source'][\"registered_domain\"]\n",
    "# text = results[0]['_source'][\"text\"]\n",
    "# # print(uid)\n",
    "# # print(url)\n",
    "# print(source)\n",
    "# print(credibility)\n",
    "# print(domain)\n",
    "# print(text)\n",
    "\n",
    "# len_returned = 5000\n",
    "# results = []\n",
    "# while len_returned == 5000:\n",
    "#     res = es.search(\n",
    "#     index='emm_data', \n",
    "#     size=5000, \n",
    "#     body={'query':{'match_all':{}}}, \n",
    "#     _source=['source', 'link']\n",
    "#     )\n",
    "#     received = res['hits']['hits']\n",
    "#     results.extend(received)\n",
    "#     len_returned = len(received)\n",
    "\n",
    "# import json\n",
    "\n",
    "# # store article's id and url in json\n",
    "# def get_id_and_url(res):\n",
    "#     results = res['hits']['hits']\n",
    "# #     print(results)\n",
    "#     res_dict = {}\n",
    "#     urls = {}\n",
    "#     for r in results:\n",
    "# #         print(r)\n",
    "#         uid = r['_id']\n",
    "#         url = r['_source']['link']\n",
    "# #         source = r['_source']['source']\n",
    "#         credibility = r['_source'][\"credibility\"]\n",
    "#         domain = r['_source'][\"registered_domain\"]\n",
    "# #         text = r['_source'][\"text\"]\n",
    "# #         res_dict[uid]={\"url\": url, \"source\": source, \"credibility\": credibility, \"domain\": domain, \"text\": text}\n",
    "        \n",
    "#     # save info to a json file\n",
    "#     with open('data.json', 'w') as f:\n",
    "#         json.dump(res_dict, f)\n",
    "    \n",
    "#     return urls\n",
    "\n",
    "# print(res)\n",
    "# # urls = get_id_and_url(res)\n",
    "\n",
    "# !pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78be52e2-a691-4710-8d3b-0c05d46df939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting information from the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd5183e-a9a0-41be-970d-d6ac63310a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import newspaper\n",
    "# from newspaper import Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb87c07-1a9e-4ef8-aa5d-2fbcb4215128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # get article's url\n",
    "# url = res['hits']['hits'][0].get('_source')['link']\n",
    "\n",
    "# # create Article object\n",
    "# article = Article(url)\n",
    "\n",
    "# # download & parse the article\n",
    "# article.download()\n",
    "# article.parse()\n",
    "\n",
    "# # extract NL properties from the text\n",
    "# article.nlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6db0a0c-8f26-4552-881d-cccab3d9e086",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get author(s), publish date & top image\n",
    "f'author: {article.authors},    publication date: {article.publish_date},    top image: {article.top_image}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12bfc0d-2da6-4653-aa27-8b7f7ae819e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # get the article's text\n",
    "# article.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9099002f-5789-4457-afd2-613ff8e44669",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now perform NLP on the article's text with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b313ff2-ae30-4bcd-b5a3-ae1f90102381",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4ee901-de1b-4774-ae2c-f82bcd828dd1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939069f8-9770-4a28-9408-3390dea58227",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# store the article's text\n",
    "# article_text = article.text\n",
    "\n",
    "# # tokenize the text\n",
    "# article_doc = nlp(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18615f05-66df-44b2-85f8-7e541235fef8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # extract tokens from the doc\n",
    "# article_tokens = [token.text for token in article_doc]\n",
    "\n",
    "# # print the tokens\n",
    "# article_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a64ec46-5a49-4fe8-a990-c1e14c352969",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aba378b-88e1-419f-b54a-2ed5d45ad773",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # extract info and perfom NLP on articles\n",
    "# def extract_article_info():\n",
    "#     articles_dict = {}\n",
    "# #     counter = 0\n",
    "#     for url_id, url in urls.items():\n",
    "# #         if counter == 100:\n",
    "# #             break\n",
    "# #         else:\n",
    "# #             article = Article(url)\n",
    "# #             if article.is_valid_url():\n",
    "# #                 article.download()\n",
    "# #                 article.parse()\n",
    "# #                 articles_dict[url_id] = article.text\n",
    "        \n",
    "#         article = Article(url)\n",
    "#         if article.is_valid_url():\n",
    "#             article.download()\n",
    "#         try:    \n",
    "#             article.parse()\n",
    "#         except:\n",
    "#             continue\n",
    "#         articles_dict[url_id] = article.text\n",
    "# #         counter+=1\n",
    "    \n",
    "#     return articles_dict\n",
    "\n",
    "# articles_dict = extract_article_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d18232-053d-497b-b026-c8acedca4258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(extract_article_info().items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67094fb-542e-4a91-95d8-62c5e9283d2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "import numpy as np\n",
    "import scipy.stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef3c14a-016a-4424-9766-f97e6aae8b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob')\n",
    "\n",
    "articles_attr = {}\n",
    "subjectivity_vals = np.empty(shape=[0, 1])\n",
    "polarity_vals = np.empty(shape=[0, 1])\n",
    "id_legend = np.empty(shape=[0, 1])\n",
    "\n",
    "for key, value in articles_dict.items():\n",
    "    doc = nlp(value)\n",
    "    articles_attr[key] = [doc._.polarity, doc._.subjectivity, doc._.assessments]\n",
    "    subjectivity_vals = np.append(subjectivity_vals, doc._.subjectivity)\n",
    "    polarity_vals = np.append(polarity_vals, doc._.polarity)\n",
    "    id_legend = np.append(id_legend, key)\n",
    "\n",
    "subjectivity_mean = np.mean(subjectivity_vals)\n",
    "subjectivity_sd = np.std(subjectivity_vals)\n",
    "polarity_mean = np.mean(polarity_vals)\n",
    "polarity_sd = np.std(polarity_vals)\n",
    "\n",
    "id_legend_save = np.copy(id_legend)\n",
    "\n",
    "excluded = []\n",
    "excluded_index = []\n",
    "\n",
    "for i in range(len(id_legend_save)):\n",
    "    if (subjectivity_vals[i] > subjectivity_mean + 2.5*subjectivity_sd) or (subjectivity_vals[i] < subjectivity_mean - 2.5*subjectivity_sd) or (polarity_vals[i] > polarity_mean + 2.5*polarity_sd) or (polarity_vals[i] < polarity_mean - 2.5*polarity_sd):\n",
    "        excluded.append(id_legend_save[i])\n",
    "        excluded_index.append(i)\n",
    "        \n",
    "subjectivity_vals = np.delete(subjectivity_vals, excluded_index)\n",
    "polarity_vals = np.delete(polarity_vals, excluded_index)\n",
    "id_legend = np.delete(id_legend, excluded_index)\n",
    "\n",
    "    \n",
    "#subjectivity: 0.0 = objective, 1.0 = subjective\n",
    "#polarity: measures level of approval/disapproval\n",
    "#assessments = list of polarity, subjectivity scores for assessed tokens\n",
    "\n",
    "\n",
    "#x-y plot of subjectivity, polarity\n",
    "#see if there is any relationship, clustering\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def do_plot(subjectivity_vals, polarity_vals, tags):\n",
    "    # Plot\n",
    "    plt.scatter(subjectivity_vals, polarity_vals, label=\"oridinal data\")\n",
    "    plt.xlabel(\"Subjectivity\")\n",
    "    plt.ylabel(\"Polarity\")\n",
    "    plt.title(\"Subjectivity-Polarity Word-level\")\n",
    "    plt.legend()\n",
    "    m, b = np.polyfit(subjectivity_vals, polarity_vals, 1) #regression\n",
    "    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(subjectivity_vals,  polarity_vals)\n",
    "    with open('stats.txt', 'w') as f:\n",
    "        f.write(\"slope: \", slope)\n",
    "        f.write(\"intercept: \", intercept)\n",
    "        f.write(\"r_value: \", r_value)\n",
    "        f.write(\"p_value: \", p_value)\n",
    "        f.write(\"std_err: \", std_err)\n",
    "    plt.plot(subjectivity_vals, m*subjectivity_vals + b)\n",
    "    plt.savefig(\"initial_plot.png\")\n",
    "    plt.close()\n",
    "\n",
    "do_plot(subjectivity_vals, polarity_vals, id_legend)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03289c06-bba9-49d6-9fa3-62172b40ce6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
