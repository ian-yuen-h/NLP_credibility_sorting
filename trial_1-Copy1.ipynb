{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# get articles from elastic search"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import json\n",
    "from elasticsearch.helpers import scan"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'elasticsearch'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4c8906c2db8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0melasticsearch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mElasticsearch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0melasticsearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhelpers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'elasticsearch'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "es = Elasticsearch([{'host':'localhost', 'port':9200}], http_auth=('uchicago', '3fMy7Wq90LDWBCMEXt6j'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "res = scan(\n",
    "    es,\n",
    "    index='credibility_training_data', \n",
    "    size=5000, \n",
    "    query={'query':{'match_all':{}}}, \n",
    ")\n",
    "\n",
    "res_dict = {}\n",
    "counter = 0\n",
    "for each in res:\n",
    "    uid = each['_id']\n",
    "    url = each['_source']['link']\n",
    "    credibility = each['_source'][\"credibility\"]\n",
    "    domain = each['_source'][\"registered_domain\"]\n",
    "    try:\n",
    "        text = each['_source'][\"text\"]\n",
    "        source = each['_source']['source']\n",
    "    except:\n",
    "        continue\n",
    "    res_dict[uid]={\"url\": url, \"source\": source, \"credibility\": credibility, \"domain\": domain, \"text\": text}\n",
    "    if counter == 0:\n",
    "        print(each)\n",
    "        counter += 1\n",
    "\n",
    "print(len(res_dict))\n",
    "        \n",
    "#     # save info to a json file\n",
    "with open('data.json', 'w') as f:\n",
    "    json.dump(res_dict, f)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'_index': 'credibility_training_data', '_type': '_doc', '_id': '204f5d99-c9de-4bce-ae2f-5e98085b4930', '_score': None, '_source': {'link': 'https://gazette.com/election-coverage/q-a-s-with-the-colorado-gubernatorial-candidates/article_37967414-d940-11e8-986a-972c594bffd9.html', 'source': 'gazette', 'language': 'en', 'credibility': 'medium', 'registered_domain': 'gazette.com', 'text': 'Here are answers to questions we sent to all four major- and minor-party governor candidates in November’s election.\\n\\nBill Hammons, Unity Party\\n\\nLt. Gov. running mate: Eric Bodenstab\\n\\nWhat makes you a better choice than your opponents?\\n\\nI’m the best choice for the simple reason that I’m truly something new under the sun (for once). I founded (with my father) the now-38-state Unity Party of America the day after the 2004 Election, and founded the Unity Party of Colorado by personally gathering well over 1,000 valid voter signatures in multiple statewide races, placing myself as a Unity Party candidate on the General Election ballot. Those 1,000 signatures put the Unity Party on the voter registration form as the “Unity” option, and the 1,000 voters who subsequently affiliated with “Unity” put UP into official party status in Colorado.\\n\\nRank in priority the top three issues you want to work on, and explain your first choice.\\n\\nColorado can and should lead the way as a state example in fighting climate change, in providing all citizens (through Medicare-inspired means) with the health care they need and deserve, and in growing and developing a sustainable economy. Climate change and its effects are real, are man-made, are a real threat, and can be fought. We need to move our state’s taxation system away from punishing workers for feeding their families, and towards discouraging destructive behavior of all kinds. Such a system would be a more efficient and constructive model for the national sales tax long advocated by many conservatives.\\n\\nThe Taxpayer’s Bill of Rights, or TABOR, gets a lot of criticism. Does it need fixing?\\n\\nTABOR doesn’t need fixing; it needs replacing with laws that better respect and acknowledge the intelligence of voters. Telling local governments what they can and cannot do with their own revenues is inappropriate (note that I oppose some forms of local taxation on principle, but to effectively tell local voters how much they choose to legally tax themselves is like outlawing exercise). The approval of TABOR implies that the lawmakers representing the two-party duopoly (and not representing their constituents) cannot be trusted to productively spend money, and I wholeheartedly agree; it’s time to finally outlaw gerrymandering at all legislative levels.\\n\\nScott Helker, Libertarian Party\\n\\nLt. Gov. running mate: Michele Poague\\n\\nWhat makes you a better choice than your opponents?\\n\\nThe biggest difference is that I do not think of government as having the only solutions. Because I do not put myself in a box where only 10 percent the solutions are, I am free to explore the other 90 percent of the solutions that are available.\\n\\nRank in priority the top three issues you want to work on, and explain your first choice.\\n\\nMy three issues begin with water. The solutions put forward by both Mr. Stapleton and Mr. Polis will not increase the volume of water in the Colorado river basins or refill the aquifers in Colorado and by default the aquifers in the surrounding states.\\n\\nThe Taxpayer’s Bill of Rights, or TABOR, gets a lot of criticism. Does it need fixing?\\n\\nTABOR does not need fixing! Not because there is unjustified criticism of TABOR, but because what choice do we have? Puerto Rico has already declared bankruptcy. In the next decade Illinois, California and New York must follow. Worse, how long before we must pay off the 21 trillion dollars of Federal debt, and how? Until we can elect legislators to the State Assembly and Senate who are fiscally responsible, TABOR will need to stay in place as is! In fact, I would encourage the other 49 states and the Federal Government to pass TABOR-like amendments so that their economies could be as strong as Colorado’s, and they would not need to file bankruptcy.\\n\\nJared Polis, Democrat\\n\\nLt. Gov. running mate: Dianne Primavera\\n\\nWhat makes you a better choice than your opponents?\\n\\nI’ve started businesses, created hundreds of jobs, and built schools for at-risk youth. I’m especially proud to have helped launch Patriot Boot Camp, a mentorship program that has helped veterans and military families launch their own small businesses. My 100-day health care plan will reduce premiums, stop prescription price gouging, and expand access to quality care across Colorado. Walker Stapleton has said he will roll back the Medicaid expansion and get rid of Colorado’s health exchange — throwing hundreds of thousands of Coloradans off their insurance and driving up costs for the rest of us.\\n\\nRank in priority the top three issues you want to work on, and explain your first choice.\\n\\n1) Tackling the rising cost of living so Coloradans can afford to live and raise their families here\\n\\n2) Building a world-class public education system\\n\\n3) Expanding access to high-quality affordable health care\\n\\nFar too many Coloradans today feel like they can’t get ahead despite our booming economy. Paychecks have barely budged, but between housing, health care, school, and daycare, the cost of living has gotten out of control. We need to build a Colorado economy where people can not just get by, but thrive — whether it’s the farmer whose livelihood is in danger due to drought, the young professional whose income isn’t keeping up with the rent, or the parents struggling to pay their health premium on top of the staggering cost of preschool.\\n\\nThe Taxpayer’s Bill of Rights, or TABOR, gets a lot of criticism. Does it need fixing?\\n\\nAny reforms to TABOR will have to be done in a bipartisan way. I would oppose changing the provision in TABOR that gives voters a say over new taxes. As governor, I will convene Republicans, Democrats and the business community to fix the growth formula and provide more flexibility so that we can invest in important priorities like public schools and transportation.\\n\\nWalker Stapleton, Republican\\n\\nLt. Gov. running mate: Lang Sias\\n\\nWhat makes you a better choice than your opponents?\\n\\nI believe my track record of working in a bipartisan way to defend the taxpayers of Colorado puts me in the best position to serve as our next governor. Just two years ago, former Democratic Governor Bill Ritter and I were co-chairs of the effort to defeat Amendment 69. This disastrous plan for a government-run, single-payer health care scheme was defeated with nearly 80 percent of the population joining us and voting no. My opponent has embraced this policy for rationed, low-quality health care. His plan for single-payer health care, along with his plan for 100 percent renewable energy are simply too expensive and too radical for Colorado.\\n\\nRank in priority the top three issues you want to work on, and explain your first choice.\\n\\nMy top priorities are: health care, transportation, and attainable housing. All of these issues boil down to a single concern; managing our growth. Colorado is in the midst of an economic boom, but this boom has come with negative side effects that impact everyone in our state. With the hundreds of thousands of new residents have come deteriorating roads and skyrocketing price increases for housing and health care. These factors are detracting from our quality of life and eating into the economic gains that we have seen from our growth. We must solve our transportation woes without raising taxes.\\n\\nThe Taxpayer’s Bill of Rights, or TABOR, gets a lot of criticism. Does it need fixing?\\n\\nI believe that TABOR has been a critical component to our state’s economic growth since it became part of Colorado’s Constitution. Every household must balance their own budget and I think it is critical that our state government does the same. Giving voters a voice in how much of their hard-earned dollars government can take and spend is a safeguard against out-of-control tax and spend policies that have overrun states across the country. This amendment has created a government that is more accountable and responsive to the will of taxpayers.'}, 'sort': [0]}\n",
      "40681\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "\n",
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "import numpy as np\n",
    "import scipy.stats\n"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob')\n",
    "\n",
    "articles_attr = {}\n",
    "subjectivity_vals = np.empty(shape=[0, 1])\n",
    "polarity_vals = np.empty(shape=[0, 1])\n",
    "cred_score = np.empty(shape=[0, 1])\n",
    "id_legend = np.empty(shape=[0, 1])\n",
    "publishers_vals = np.empty(shape=[0, 1])\n",
    "publisher_hashes = {}\n",
    "\n",
    "# subjectivity_vals_low = np.empty(shape=[0, 1])\n",
    "# polarity_vals_low = np.empty(shape=[0, 1])\n",
    "# subjectivity_vals_med = np.empty(shape=[0, 1])\n",
    "# polarity_vals_med = np.empty(shape=[0, 1])\n",
    "# subjectivity_vals_high = np.empty(shape=[0, 1])\n",
    "# polarity_vals_high = np.empty(shape=[0, 1])\n",
    "# polarity_vals_high = np.empty(shape=[0, 1])\n",
    "\n",
    "labeled_scores = {}\n",
    "counter = 0\n",
    "\n",
    "for key, value in res_dict.items():\n",
    "    doc = nlp(value[\"text\"])\n",
    "    articles_attr[key] = {\"polarity\": doc._.polarity, \"subjectivity\": doc._.subjectivity, \"assessments\": doc._.assessments, \"source\": value[\"source\"]}\n",
    "    subjectivity_vals = np.append(subjectivity_vals, doc._.subjectivity)\n",
    "    polarity_vals = np.append(polarity_vals, doc._.polarity)\n",
    "    id_legend = np.append(id_legend, key)\n",
    "    credi_score = value[\"credibility\"]\n",
    "    cred_score = np.append(cred_score, credi_score)\n",
    "    labeled_scores[key]= credi_score\n",
    "    source_h = None\n",
    "    try:\n",
    "        source_h = publisher_hashes[value[\"source\"]]\n",
    "    except KeyError:\n",
    "        publisher_hashes[value[\"source\"]] = counter\n",
    "        source_h = counter\n",
    "        counter += 1\n",
    "    finally:\n",
    "        publishers_vals = np.append(publishers_vals, source_h)\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "#excluding outliers\n",
    "subjectivity_mean = np.mean(subjectivity_vals)\n",
    "subjectivity_sd = np.std(subjectivity_vals)\n",
    "polarity_mean = np.mean(polarity_vals)\n",
    "polarity_sd = np.std(polarity_vals)\n",
    "\n",
    "id_legend_save = np.copy(id_legend)\n",
    "\n",
    "excluded = []\n",
    "excluded_index = []\n",
    "\n",
    "upper_subjectivity = subjectivity_mean + 3*subjectivity_sd\n",
    "lower_subjectivity = subjectivity_mean - 3*subjectivity_sd\n",
    "upper_polarity = polarity_mean + 3*polarity_sd\n",
    "lower_polarity = polarity_mean - 3*polarity_sd\n",
    "\n",
    "for i in range(len(id_legend_save)):\n",
    "    if (subjectivity_vals[i] > upper_subjectivity) or (subjectivity_vals[i] < lower_subjectivity) or (polarity_vals[i] > upper_polarity) or (polarity_vals[i] < lower_polarity):\n",
    "        excluded.append(id_legend_save[i])\n",
    "        excluded_index.append(i)\n",
    "        \n",
    "subjectivity_vals = np.delete(subjectivity_vals, excluded_index)\n",
    "polarity_vals = np.delete(polarity_vals, excluded_index)\n",
    "id_legend = np.delete(id_legend, excluded_index)\n",
    "cred_score = np.delete(cred_score, excluded_index)\n",
    "\n",
    "#write out the vectors, excluded or no?\n",
    "with open('vectors.txt', 'w') as f:\n",
    "    for i in range(len(cred_score)):\n",
    "        f.write(\"id: \")\n",
    "        f.write(id_legend[i].astype('str'))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"polarity: \")\n",
    "        f.write(polarity_vals[i].astype('str'))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"subjectivity: \")\n",
    "        f.write(subjectivity_vals[i].astype('str'))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"cred_score: \")\n",
    "        f.write(cred_score[i].astype('str'))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"publisher_val: \")\n",
    "        f.write(publishers_vals[i].astype('str')])\n",
    "        f.write(str(publisher_hashes[res_dict[id_legend[i]][\"source\"]]))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"publisher: \")\n",
    "        f.write(res_dict[id_legend[i]][\"source\"])\n",
    "        f.write()\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"-------------\\n\")\n",
    "        #add publisher to above and here\n",
    "    \n",
    "\n",
    "print(\"number of excluded: \", len(excluded_index))\n",
    "print(\"number of included: \", len(id_legend))\n",
    "\n",
    "with open('publisher_hash.txt', 'w') as f:\n",
    "    for key, value in publisher_hashes.items():\n",
    "        f.write(\"Publisher: \")\n",
    "        f.write(str(key))\n",
    "        f.write(\" , hashed: \")\n",
    "        f.write(str(value))\n",
    "        f.write(\"\\n\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "number of excluded:  59\n",
      "number of included:  38867\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "def do_plot(subjectivity_vals, polarity_vals, tags, cred_score):\n",
    "    # Plot\n",
    "#     colors = [\"red\", \"green\", \"blue\"]\n",
    "    cdict = {\"high\": 'red', \"medium\": 'blue', \"low\": 'green'}\n",
    "#     fig = plt.figure(figsize=(8,8))\n",
    "    fig, ax = plt.subplots(figsize=(20,20))\n",
    "    for g in np.unique(cred_score):\n",
    "        ix = np.where(cred_score == g)\n",
    "        ax.scatter(subjectivity_vals[ix], polarity_vals[ix], c = cdict[g], label = g)\n",
    "    ax.legend()\n",
    "#     plt.scatter(subjectivity_vals, polarity_vals, c = cdict[g])\n",
    "    plt.xlabel(\"Subjectivity\")\n",
    "    plt.ylabel(\"Polarity\")\n",
    "    plt.title(\"Subjectivity-Polarity Word-level\")\n",
    "    plt.legend()\n",
    "    m, b = np.polyfit(subjectivity_vals, polarity_vals, 1) #regression\n",
    "    plt.plot(subjectivity_vals, m*subjectivity_vals + b)\n",
    "    plt.savefig(\"initial_plot.png\")\n",
    "    plt.close()\n",
    "\n",
    "do_plot(subjectivity_vals, polarity_vals, id_legend, cred_score)\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(subjectivity_vals,  polarity_vals)\n",
    "with open('stats.txt', 'w') as f:\n",
    "    f.write(\"slope: \")\n",
    "    f.write(slope.astype('str'))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"intercept: \")\n",
    "    f.write(intercept.astype('str'))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"r_value: \")\n",
    "    f.write(r_value.astype('str'))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"p_value: \")\n",
    "    f.write(p_value.astype('str'))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"std_err: \")\n",
    "    f.write(std_err.astype('str'))\n",
    "    f.write(\"-------------\\n\")"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# # print(res.to_dict)\n",
    "# counter = 0\n",
    "# for each in res:\n",
    "#     if counter == 3:\n",
    "#         break\n",
    "#     print(each)\n",
    "#     counter += 1\n",
    "\n",
    "\n",
    "# results = []\n",
    "\n",
    "# res = es.search(\n",
    "#     index='credibility_training_data', \n",
    "#     scroll = '2m',\n",
    "#     search_type = 'scan',\n",
    "#     size=5000, \n",
    "#     body={'query':{'match_all':{}}}, \n",
    "# )\n",
    "\n",
    "# # get and return a list of urls\n",
    "# def get_urls():\n",
    "#     results = res['hits']['hits']\n",
    "#     urls = {}\n",
    "#     for u in results:\n",
    "#         url_id = u['_id']\n",
    "#         url = u['_source']['link']\n",
    "#         urls[url_id] = url\n",
    "#     return urls\n",
    "    \n",
    "# urls = get_urls()\n",
    "# len(urls)\n",
    "\n",
    "# res = es.search(\n",
    "#     index='credibility_training_data', \n",
    "#     size=5000, \n",
    "#     body={'query':{'match_all':{}}}, \n",
    "#     _source=['source', 'link']\n",
    "# )\n",
    "\n",
    "\n",
    "# results = res['hits']['hits']\n",
    "# print(len(results))\n",
    "#     counter += 1\n",
    "    \n",
    "# uid = results[0]['_id']\n",
    "# url = results[0]['_source']['link']\n",
    "# source = results[0]['_source']['source']\n",
    "# credibility = results[0]['_source'][\"credibility\"]\n",
    "# domain = results[0]['_source'][\"registered_domain\"]\n",
    "# text = results[0]['_source'][\"text\"]\n",
    "# # print(uid)\n",
    "# # print(url)\n",
    "# print(source)\n",
    "# print(credibility)\n",
    "# print(domain)\n",
    "# print(text)\n",
    "\n",
    "# len_returned = 5000\n",
    "# results = []\n",
    "# while len_returned == 5000:\n",
    "#     res = es.search(\n",
    "#     index='emm_data', \n",
    "#     size=5000, \n",
    "#     body={'query':{'match_all':{}}}, \n",
    "#     _source=['source', 'link']\n",
    "#     )\n",
    "#     received = res['hits']['hits']\n",
    "#     results.extend(received)\n",
    "#     len_returned = len(received)\n",
    "\n",
    "# import json\n",
    "\n",
    "# # store article's id and url in json\n",
    "# def get_id_and_url(res):\n",
    "#     results = res['hits']['hits']\n",
    "# #     print(results)\n",
    "#     res_dict = {}\n",
    "#     urls = {}\n",
    "#     for r in results:\n",
    "# #         print(r)\n",
    "#         uid = r['_id']\n",
    "#         url = r['_source']['link']\n",
    "# #         source = r['_source']['source']\n",
    "#         credibility = r['_source'][\"credibility\"]\n",
    "#         domain = r['_source'][\"registered_domain\"]\n",
    "# #         text = r['_source'][\"text\"]\n",
    "# #         res_dict[uid]={\"url\": url, \"source\": source, \"credibility\": credibility, \"domain\": domain, \"text\": text}\n",
    "        \n",
    "#     # save info to a json file\n",
    "#     with open('data.json', 'w') as f:\n",
    "#         json.dump(res_dict, f)\n",
    "    \n",
    "#     return urls\n",
    "\n",
    "# print(res)\n",
    "# # urls = get_id_and_url(res)\n",
    "\n",
    "# !pip install newspaper3k"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Extracting information from the article"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import newspaper\n",
    "# from newspaper import Article"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# # get article's url\n",
    "# url = res['hits']['hits'][0].get('_source')['link']\n",
    "\n",
    "# # create Article object\n",
    "# article = Article(url)\n",
    "\n",
    "# # download & parse the article\n",
    "# article.download()\n",
    "# article.parse()\n",
    "\n",
    "# # extract NL properties from the text\n",
    "# article.nlp()"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get author(s), publish date & top image\n",
    "f'author: {article.authors},    publication date: {article.publish_date},    top image: {article.top_image}'"
   ],
   "outputs": [],
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# # get the article's text\n",
    "# article.text"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# now perform NLP on the article's text with spaCy"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import spacy"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ],
   "outputs": [],
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# store the article's text\n",
    "# article_text = article.text\n",
    "\n",
    "# # tokenize the text\n",
    "# article_doc = nlp(article_text)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# # extract tokens from the doc\n",
    "# article_tokens = [token.text for token in article_doc]\n",
    "\n",
    "# # print the tokens\n",
    "# article_tokens"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# # extract info and perfom NLP on articles\n",
    "# def extract_article_info():\n",
    "#     articles_dict = {}\n",
    "# #     counter = 0\n",
    "#     for url_id, url in urls.items():\n",
    "# #         if counter == 100:\n",
    "# #             break\n",
    "# #         else:\n",
    "# #             article = Article(url)\n",
    "# #             if article.is_valid_url():\n",
    "# #                 article.download()\n",
    "# #                 article.parse()\n",
    "# #                 articles_dict[url_id] = article.text\n",
    "        \n",
    "#         article = Article(url)\n",
    "#         if article.is_valid_url():\n",
    "#             article.download()\n",
    "#         try:    \n",
    "#             article.parse()\n",
    "#         except:\n",
    "#             continue\n",
    "#         articles_dict[url_id] = article.text\n",
    "# #         counter+=1\n",
    "    \n",
    "#     return articles_dict\n",
    "\n",
    "# articles_dict = extract_article_info()"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# print(extract_article_info().items())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "import numpy as np\n",
    "import scipy.stats\n"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe('spacytextblob')\n",
    "\n",
    "articles_attr = {}\n",
    "subjectivity_vals = np.empty(shape=[0, 1])\n",
    "polarity_vals = np.empty(shape=[0, 1])\n",
    "id_legend = np.empty(shape=[0, 1])\n",
    "\n",
    "for key, value in articles_dict.items():\n",
    "    doc = nlp(value)\n",
    "    articles_attr[key] = [doc._.polarity, doc._.subjectivity, doc._.assessments]\n",
    "    subjectivity_vals = np.append(subjectivity_vals, doc._.subjectivity)\n",
    "    polarity_vals = np.append(polarity_vals, doc._.polarity)\n",
    "    id_legend = np.append(id_legend, key)\n",
    "\n",
    "subjectivity_mean = np.mean(subjectivity_vals)\n",
    "subjectivity_sd = np.std(subjectivity_vals)\n",
    "polarity_mean = np.mean(polarity_vals)\n",
    "polarity_sd = np.std(polarity_vals)\n",
    "\n",
    "id_legend_save = np.copy(id_legend)\n",
    "\n",
    "excluded = []\n",
    "excluded_index = []\n",
    "\n",
    "for i in range(len(id_legend_save)):\n",
    "    if (subjectivity_vals[i] > subjectivity_mean + 2.5*subjectivity_sd) or (subjectivity_vals[i] < subjectivity_mean - 2.5*subjectivity_sd) or (polarity_vals[i] > polarity_mean + 2.5*polarity_sd) or (polarity_vals[i] < polarity_mean - 2.5*polarity_sd):\n",
    "        excluded.append(id_legend_save[i])\n",
    "        excluded_index.append(i)\n",
    "        \n",
    "subjectivity_vals = np.delete(subjectivity_vals, excluded_index)\n",
    "polarity_vals = np.delete(polarity_vals, excluded_index)\n",
    "id_legend = np.delete(id_legend, excluded_index)\n",
    "\n",
    "    \n",
    "#subjectivity: 0.0 = objective, 1.0 = subjective\n",
    "#polarity: measures level of approval/disapproval\n",
    "#assessments = list of polarity, subjectivity scores for assessed tokens\n",
    "\n",
    "\n",
    "#x-y plot of subjectivity, polarity\n",
    "#see if there is any relationship, clustering\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def do_plot(subjectivity_vals, polarity_vals, tags):\n",
    "    # Plot\n",
    "    plt.scatter(subjectivity_vals, polarity_vals, label=\"oridinal data\")\n",
    "    plt.xlabel(\"Subjectivity\")\n",
    "    plt.ylabel(\"Polarity\")\n",
    "    plt.title(\"Subjectivity-Polarity Word-level\")\n",
    "    plt.legend()\n",
    "    m, b = np.polyfit(subjectivity_vals, polarity_vals, 1) #regression\n",
    "    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(subjectivity_vals,  polarity_vals)\n",
    "    with open('stats.txt', 'w') as f:\n",
    "        f.write(\"slope: \", slope)\n",
    "        f.write(\"intercept: \", intercept)\n",
    "        f.write(\"r_value: \", r_value)\n",
    "        f.write(\"p_value: \", p_value)\n",
    "        f.write(\"std_err: \", std_err)\n",
    "    plt.plot(subjectivity_vals, m*subjectivity_vals + b)\n",
    "    plt.savefig(\"initial_plot.png\")\n",
    "    plt.close()\n",
    "\n",
    "do_plot(subjectivity_vals, polarity_vals, id_legend)\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}